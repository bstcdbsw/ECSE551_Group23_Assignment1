{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f8e365-01f3-4e4f-9e2d-e6ae13361982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "class MLP():\n",
    "    #define constructive function to init class MLP\n",
    "    def __init__(self, input_size, output_size,lr):\n",
    "        self.input_size=input_size\n",
    "        self.output_size=output_size\n",
    "        self.hidden_layer_size=lr\n",
    "\n",
    "    #define relu activation function\n",
    "    def relu(self, Z):\n",
    "        return max(Z,0)\n",
    "\n",
    "    #define relu_derivative function\n",
    "    def relu_derivative(self, Z):\n",
    "        return 1 if Z>0 else 0\n",
    "\n",
    "    def linear(self,Z):\n",
    "        return Z\n",
    "\n",
    "    #define sigmoid activation function\n",
    "    def sigmoid(self,Z):\n",
    "        return 1/(1+math.exp(-Z))\n",
    "\n",
    "    #define softmax activation function\n",
    "    def softmax(self,Z):\n",
    "        ez = np.exp(z)\n",
    "        sm = ez/np.sum(ez)\n",
    "        return sum\n",
    "\n",
    "    #define mean squared error loss function\n",
    "    def mean_squared_error():\n",
    "        return\n",
    "\n",
    "    #define binary cross entropy loss function\n",
    "    def binary_cross_entropy():\n",
    "        return\n",
    "\n",
    "    def sparse_categorical_cross_entropy(z,label)\n",
    "        return\n",
    "        \n",
    "    def dense(self,X,W,b,activation_func):\n",
    "        units = W.shape[1]\n",
    "        y = np.zeros(units)\n",
    "        for j in range(units):\n",
    "            w=W[:,j]\n",
    "            z=np.dot(w,X)+b[j]\n",
    "            if activation_func == 'relu':\n",
    "                y[j]=self.relu(z)\n",
    "            elif activation_func == 'relu_derivative':\n",
    "                y[j]=self.relu_derivative(z)\n",
    "            elif activation_func == 'sigmoid':\n",
    "                y[j]=self.sigmoid(z)\n",
    "            else \n",
    "                y[j]=self.linear(z)\n",
    "        return y  \n",
    "        \n",
    "    def sequential(self,x):\n",
    "        W1,b1=np.array.random.rand(self.input_size)\n",
    "        W2,b2=np.array.random.rand(self.hidden_layer_size)\n",
    "        W3,b3=np.array.random.rand(self.output_size)\n",
    "        a1 = self.dense(x,W1,b1,'relu')\n",
    "        a2 = self.dense(a1,W2,b2,'relu')\n",
    "        a3 = self.dense(a2,W3,b3,'linear')\n",
    "\n",
    "    def fit(self, X, y, epochs):\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23004abd-99a8-48be-b81b-694283c62363",
   "metadata": {},
   "source": [
    "in softmax, is Z a vector or scalar?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a112a186-0e81-46a4-a9c5-de2bca596190",
   "metadata": {},
   "source": [
    "We should define every layer's dense. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
